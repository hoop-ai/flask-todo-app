# Flask Todo App V&V Mega Context

## 1. Project Snapshot
- Course: SEN4013 Software Verification & Validation
- Deliverables: automated test suite, V&V report, presentation packet, execution artifacts
- Status: Test assets prepared; report/presentation need names, visuals, screenshots inserted
- Target presentation date: January 16, 2026; team fields remain placeholders for you to fill
- Core objective: demonstrate comprehensive V&V of a Flask-based todo application using multiple verification techniques

## 2. System Under Test
- Application: Flask Todo App (`app.py`) with web UI, REST API at `/api/todos`, Swagger docs at `/apidocs`, SQLite persistence
- Tech stack: Flask 1.1.1, Flask-SQLAlchemy 2.4.1, Flasgger 0.9.7.1, Python 3.12, Playwright for UI automation
- Why chosen: realistic microservice footprint (UI + API + DB) with clear Swagger specification and multiple testing surfaces

## 3. Verification & Validation Scope
- Verification question: Are we building the product right? Validate implementation against specification via automated checks
- Validation question: Are we building the right product? Confirm UI behaviour matches user expectations through E2E tests
- Evidence collected via functional tests, property-based tests, negative tests, runtime contracts, formal verification, UI/E2E, and coverage analysis

## 4. Toolchain Overview
| Tool | Purpose | Notes |
|------|---------|-------|
| pytest | Functional/unit testing | Handles base assertions and discovery |
| Hypothesis | Property-based testing | Generates 100+ edge-case inputs automatically |
| coverage.py | Structural coverage | Terminal, XML, and HTML reports; 67% overall |
| Playwright | UI/E2E automation | Drives Chromium, captures screenshot evidence |
| icontract | Runtime contracts | Enforces pre/postconditions in `domain_contracts.py` |
| CrossHair | Solver-backed verification | Uses SMT to search for contract violations |

Supporting files include `Makefile` (automation), `VV_README.md` (setup/troubleshooting), and `PROJECT_STRUCTURE.md` (file map).

## 5. Test Inventory & Results
| ID | Area | Type | Status | Highlights |
|----|------|------|--------|------------|
| T01 | Home UI load | Functional | Pass | `GET /` returns 200 |
| T02 | Create/List roundtrip | Property-based | Pass | Hypothesis generated 100+ cases |
| T03 | Empty title rejection | Negative | Pass | API returns 400 for blank title |
| T04 | Excessive title length | Negative | **Fail** | Bug: 200-char titles accepted (INC-001) |
| T05 | Title normalization | Contract | Pass | icontract enforces 1-40 chars post-trim |
| T06 | CrossHair check | Formal | Pass | No counterexamples found |
| T07 | UI add todo | UI/E2E | Pass | Playwright adds "Buy milk" and saves screenshot |

Overall: 6 passing checks, 1 expected failure documenting the defect. JUnit output stored in `artifacts/junit.xml`.

## 6. Key Finding (INC-001)
- Symptom: API accepts arbitrarily long titles despite DB schema hinting at 100-character limit
- Repro: POST `/api/todos` with 200-character title (see Test T04)
- Expected: 400 Bad Request with validation message
- Actual: 201 Created; potential silent truncation in DB layer
- Recommendation: add explicit length validation before persistence, update tests, re-run suite

## 7. Coverage Metrics
```
Name            Stmts  Miss  Cover
app.py             95    45    53%
tests/*            35     0   100%
domain_contracts    6     0   100%
TOTAL             136    45    67%
```
- HTML dashboard at `htmlcov/index.html`
- Required screenshot: save as `screenshots/coverage_dashboard.png`
- Optional detail view: `app.py` coverage page for uncovered lines

## 8. How to Run Everything Locally
1. `cd flask-todo-app`
2. Activate venv: `.venv\Scripts\activate` (Windows) or `source .venv/bin/activate`
3. Terminal A: `python app.py` to start Flask at http://127.0.0.1:5000
4. Terminal B: choose either
   - `make all`
   - or run individually: `pytest -v`, `coverage run -m pytest`, `coverage report -m`, `coverage html`, `crosshair check domain_contracts.py`
5. UI test alone: `pytest tests/test_ui_playwright.py -v`
6. Stop Flask with `Ctrl+C` in Terminal A when finished

Common troubleshooting (see `VV_README.md`): ensure browsers installed via `python -m playwright install`, delete stale `.pytest_cache` if needed, confirm `.venv` is active for Playwright dependencies.

## 9. Automation Shortcuts (Makefile)
- `make run` ? start Flask server
- `make test` ? execute pytest suite
- `make cov` ? collect coverage + generate HTML report
- `make ui` ? run Playwright UI test only
- `make formal` ? execute CrossHair against contracts
- `make all` ? run tests, coverage, formal check, and package artifacts

## 10. Artifacts & Evidence
- `artifacts/coverage.xml` ? Cobertura coverage data
- `artifacts/junit.xml` ? JUnit test report with expected failure for T04
- `artifacts/ui_after_add.png` ? Browser screenshot after adding "Buy milk"
- `htmlcov/index.html` ? Interactive coverage dashboard
- Recommended screenshots you capture:
  - `screenshots/coverage_dashboard.png`
  - `screenshots/pytest_output.png`
  - Optional: `screenshots/swagger_ui.png`, `screenshots/app_ui_home.png`

## 11. Directory Highlights
- Documentation: `docs/START_HERE.md`, `docs/QUICK_REFERENCE.md`, `docs/SUBMISSION_GUIDE.md`, `docs/VV_README.md`, `docs/PROJECT_STRUCTURE.md`, `docs/REPORT.md`
- Slides: `slides/SLIDES.md`, `slides/SLIDES_CONCISE.md`, `slides/SLIDES_ENHANCED.md`
- Tests: `tests/test_api_properties.py`, `tests/test_contracts.py`, `tests/test_ui_playwright.py`
- Contracts: `domain_contracts.py`
- App source: `app.py`, `templates/index.html`
- Virtual environment: `.venv/` (exclude from submission)
- Database: `todo.db` (auto-generated)

## 12. Submission & Presentation Checklist
- Update names/dates in `docs/REPORT.md` and `slides/SLIDES.md`
- Insert screenshots (coverage dashboard, pytest output, UI evidence) in report/presentation
- Export report and slides to PDF (`docs/REPORT.pdf`, `docs/slides/SLIDES.pdf` or preferred location)
- Package required files: code, tests, artifacts, docs, screenshots (exclude `.venv`, caches)
- Prepare to discuss INC-001, coverage gaps, and future work during presentation
- Practice narrative using SLIDES speaker notes (approx. 5 minutes per presenter)

## 13. Using This Mega File
- Sections 1-12 summarise the project essentials for quick reference or for prompting another AI assistant
- Appendices below embed the full report and slide outline so downstream tools can ingest the verbatim source material
- For additional setup detail or troubleshooting, reference `VV_README.md`; for file locations, see `PROJECT_STRUCTURE.md`

---
## Full Report Source (REPORT.md)
# SEN4013 Software Verification & Validation ‚Äî Flask Todo App

**Team:** [Your Names Here]
**Date:** January 16, 2026

---

## 1. Introduction

This report documents the Verification & Validation (V&V) activities performed on a Flask-based Todo application. The primary goal of V&V is to build confidence that the application meets its specifications (verification) and user needs (validation) through automated, repeatable testing and analysis.

**V&V Objectives:**
- Verify functional correctness of REST API endpoints
- Validate UI functionality through end-to-end testing
- Measure structural code coverage
- Apply property-based testing for edge cases
- Enforce runtime contracts for business rules
- Use solver-aided verification for formal guarantees

---

## 2. System Under Test (SUT)

**Application:** Flask Todo App
**Repository:** https://github.com/onurtacc/flask-todo-app
**Description:** A simple web-based todo list application with:
- **Web UI** for managing tasks (add, edit, complete, delete)
- **REST API** (`/api/todos`) for programmatic access
- **Swagger UI** at `/apidocs` for API documentation
- **SQLite Database** for persistence

**Technology Stack:**
- Flask 1.1.1 (web framework)
- Flask-SQLAlchemy 2.4.1 (ORM)
- Flasgger 0.9.7.1 (Swagger/OpenAPI integration)
- SQLite (database)

---

## 3. V&V Approach & Tools

### 3.1 Toolchain Overview

| Tool | Purpose | Citation |
|------|---------|----------|
| **pytest** | Unit and functional test framework with simple assertions and automatic test discovery | [docs.pytest.org](https://docs.pytest.org/) |
| **Hypothesis** | Property-based testing that automatically generates edge-case inputs based on specification patterns | [hypothesis.readthedocs.io](https://hypothesis.readthedocs.io/) |
| **coverage.py** | Structural code coverage measurement (statement, branch); outputs terminal, XML, and HTML reports | [coverage.readthedocs.io](https://coverage.readthedocs.io/) |
| **Playwright** | Browser automation for UI/E2E testing with screenshot capture across Chromium, Firefox, and WebKit | [playwright.dev](https://playwright.dev/python/) |
| **icontract** | Runtime design-by-contract with preconditions (`@require`) and postconditions (`@ensure`) that fail fast on violations | [icontract.readthedocs.io](https://icontract.readthedocs.io/) |
| **CrossHair** | Solver-backed formal verification that searches for contract counterexamples using symbolic execution and SMT solving | [crosshair.readthedocs.io](https://crosshair.readthedocs.io/) |

### 3.2 Test Strategy

1. **Functional Testing:** Test typical user workflows (CRUD operations) via API endpoints
2. **Property-Based Testing:** Use Hypothesis to generate hundreds of valid inputs matching regex patterns
3. **Negative Testing:** Verify proper error handling for invalid inputs
4. **UI/E2E Testing:** Automate browser interactions and capture visual evidence
5. **Structural Coverage:** Measure which code paths are exercised by tests
6. **Contract Verification:** Enforce business rules at runtime and verify with formal methods
7. **Regression Prevention:** All tests automated and repeatable in CI/CD pipelines

---

## 4. Test Cases & Results

### 4.1 Test Summary

| ID | Test Area | Type | Status | Notes |
|----|-----------|------|--------|-------|
| T01 | Home UI Load | Functional | ‚úÖ PASS | UI returns HTTP 200 |
| T02 | Create/List Roundtrip | Property-Based | ‚úÖ PASS | Hypothesis generated 100+ test cases |
| T03 | Empty Title Rejection | Negative | ‚úÖ PASS | API correctly returns 400 |
| T04 | Excessive Title Length | Negative | ‚ùå FAIL | **INCIDENT:** App accepts 200-char titles without validation |
| T05 | Contract Normalization | Contract | ‚úÖ PASS | Title normalization strips whitespace correctly |
| T06 | CrossHair Verification | Formal | ‚úÖ PASS | No counterexamples found for contract invariants |
| T07 | UI Add Todo + Screenshot | E2E | ‚úÖ PASS | Successfully added "Buy milk" via browser |

**Overall Results:** 6 passed, 1 failed (83% pass rate)

### 4.2 Detailed Test Cases

#### T01: Home UI Load
- **Input:** GET request to `/`
- **Expected:** HTTP 200, HTML page rendered
- **Actual:** HTTP 200
- **Status:** ‚úÖ PASS

#### T02: Create/List Roundtrip (Property-Based)
- **Input:** Hypothesis-generated titles (1-40 chars, alphanumeric + punctuation) and descriptions (0-120 chars)
- **Expected:** POST `/api/todos` returns 200/201, subsequent GET includes the created item
- **Actual:** All 100+ generated test cases passed
- **Status:** ‚úÖ PASS
- **Coverage:** Tests many edge cases like single-character titles, max-length titles, special characters

#### T03: Empty Title Rejection
- **Input:** POST `/api/todos` with `{"title": ""}`
- **Expected:** HTTP 400 or 422
- **Actual:** HTTP 400
- **Status:** ‚úÖ PASS

#### T04: Excessive Title Length
- **Input:** POST `/api/todos` with `{"title": "x" * 200}` (200 characters)
- **Expected:** HTTP 400 or 422
- **Actual:** HTTP 201 (created successfully)
- **Status:** ‚ùå FAIL
- **Incident:** See Section 6

#### T05: Title Normalization Contract
- **Input:** `normalize_title("  Hello  ")`
- **Expected:** Returns `"Hello"` (trimmed), no contract violations
- **Actual:** `"Hello"`
- **Status:** ‚úÖ PASS

#### T06: CrossHair Formal Verification
- **Command:** `crosshair check domain_contracts.py`
- **Expected:** No counterexamples found
- **Actual:** No output (success)
- **Status:** ‚úÖ PASS
- **Note:** CrossHair verified that `normalize_title` cannot violate its contracts for any valid input

#### T07: UI Add Todo via Browser
- **Input:** Navigate to http://127.0.0.1:5000, fill form with "Buy milk" / "2L semi-skimmed", click Add
- **Expected:** Todo appears in list, screenshot captured
- **Actual:** Todo visible in page content, screenshot saved to `artifacts/ui_after_add.png`
- **Status:** ‚úÖ PASS
- **Evidence:** See screenshot below

![UI Screenshot](artifacts/ui_after_add.png)

---

## 5. Structural Coverage

### 5.1 Coverage Summary

```
Name                           Stmts   Miss  Cover   Missing
------------------------------------------------------------
app.py                            95     45    53%   [see below]
conftest.py                        4      0   100%
domain_contracts.py                6      0   100%
tests\test_api_properties.py      28      0   100%
tests\test_contracts.py            3      0   100%
------------------------------------------------------------
TOTAL                            136     45    67%
```

**Overall Coverage:** 67%
**App Coverage:** 53%
**Test Code Coverage:** 100%

### 5.2 Uncovered Lines in app.py

The following lines were not exercised by the current test suite:

- **Lines 24-29:** Web form submission handler (`/add` POST)
- **Lines 33-36:** Complete todo via web UI (`/complete/<id>`)
- **Lines 40-43:** Incomplete todo via web UI (`/incomplete/<id>`)
- **Lines 47-50:** Delete todo via web UI (`/delete/<id>`)
- **Lines 54-59:** Edit todo via web UI (`/edit/<id>` POST)
- **Lines 115-116:** API get single todo by ID (`/api/todos/<id>` GET)
- **Line 150:** JSON validation error path in POST endpoint
- **Lines 202-214:** API update todo (`/api/todos/<id>` PUT)
- **Lines 239-242:** API delete todo (`/api/todos/<id>` DELETE)

**Analysis:**
- Current tests focus on the API create/list endpoints and basic UI load
- Web form handlers and remaining API endpoints (GET by ID, PUT, DELETE) are untested
- This represents gaps in test coverage that could hide defects

### 5.3 Coverage HTML Report

For detailed line-by-line coverage visualization, open: `htmlcov/index.html`

**Screenshot of Coverage Report:**

[PLACEHOLDER: Insert screenshot of htmlcov/index.html showing the coverage dashboard]

---

## 6. Incidents & Findings

### 6.1 Resolved Incidents

*None in this iteration*

### 6.2 Unresolved Incidents

#### INC-001: Missing Title Length Validation

**Severity:** Medium
**Status:** Open
**Test Case:** T04 - Excessive Title Length

**Description:**
The API endpoint `POST /api/todos` accepts arbitrarily long titles without validation. The database schema defines `title = db.Column(db.String(100), nullable=False)`, which suggests an intended 100-character limit, but the application code does not enforce this before attempting to store the data.

**Reproduction:**
```bash
curl -X POST http://127.0.0.1:5000/api/todos \
  -H "Content-Type: application/json" \
  -d '{"title":"'"$(python -c "print('x'*200)")"'"}'
# Returns HTTP 201 Created
```

**Expected Behavior:**
Should return HTTP 400 with error message: `{"error": "Title must be 100 characters or less"}`

**Actual Behavior:**
Returns HTTP 201, title is truncated by database (silent data loss) or causes unexpected behavior

**Impact:**
- **Data Integrity:** Potential silent truncation or database errors
- **User Experience:** No feedback when title is too long
- **Security:** Could be exploited for buffer overflow-style attacks if database layer doesn't truncate properly

**Recommendation:**
Add validation in `api_create_todo()` function ([app.py:152](app.py#L152)):

```python
if len(title) > 100:
    return jsonify({"error": "Title must be 100 characters or less"}), 400
```

**Risk if Unresolved:**
Low to Medium. SQLite will truncate to 100 chars, but users will experience unexpected behavior. Could escalate if migrated to a database that throws errors on oversized strings.

---

## 7. Limitations & Threats to Validity

### 7.1 Test Coverage Limitations

1. **Code Coverage ‚â† Test Quality:** 67% structural coverage doesn't guarantee all logical paths or edge cases are tested
2. **UI Testing Scope:** Only one UI workflow (add todo) is tested; edit, delete, and complete actions via UI are untested
3. **Property Test Scope:** Hypothesis tests use regex patterns that may not cover all real-world inputs (e.g., Unicode, emoji, SQL injection)
4. **Contract Scope:** Contracts only cover `normalize_title` function; business rules in other parts of the app are not formally specified

### 7.2 Environmental Factors

1. **Single Browser:** Playwright tests only run in Chromium; Firefox and WebKit not tested
2. **SQLite Behavior:** Database truncation behavior may differ in production databases (PostgreSQL, MySQL)
3. **No Load/Stress Testing:** Performance and concurrency issues not covered

### 7.3 Oracle Problem

- Empty title rejection and length validation expectations are derived from typical web app behavior, not formal specifications
- No formal requirements document to validate against

---

## 8. Recommendations

### 8.1 Immediate Actions

1. **Fix INC-001:** Add title length validation (< 5 minutes)
2. **Expand Coverage:** Write tests for uncovered endpoints (GET by ID, PUT, DELETE) to reach 80%+ coverage
3. **Property Test Enhancement:** Add Hypothesis strategies for Unicode, emoji, and SQL injection patterns

### 8.2 Process Improvements

1. **Continuous Integration:** Run `pytest`, `coverage`, and `crosshair` on every commit via GitHub Actions
   ```yaml
   - run: pytest --cov --junitxml=junit.xml
   - run: coverage xml
   - run: crosshair check domain_contracts.py
   ```

2. **Contract-Driven Development:** Add icontract pre/postconditions to all API endpoints:
   - `@require(lambda title: 1 <= len(title) <= 100)`
   - `@ensure(lambda result: result[1] in {200, 201, 400, 404})`

3. **Extended Property Testing:** Use Hypothesis for:
   - Stateful testing (sequences of API calls)
   - Database roundtrip properties (write then read should match)
   - Idempotency checks (PUT same data twice = same result)

4. **Formal Specification:** Document API contracts in OpenAPI/Swagger with JSON Schema validation, then auto-generate tests

### 8.3 Long-Term Strategy

1. **Mutation Testing:** Use `mutmut` to verify tests catch injected bugs
2. **Fuzzing:** Apply `atheris` to find crash-inducing inputs
3. **Model Checking:** For complex workflows, use TLA+ or Alloy to verify state machine properties

---

## 9. Conclusion

This V&V effort successfully applied multiple testing techniques to the Flask Todo application, achieving 67% code coverage and identifying one medium-severity input validation defect (INC-001). The combination of functional, property-based, contract, and formal verification methods demonstrates a comprehensive approach to building confidence in software quality.

**Key Achievements:**
- Automated test suite with 6/7 tests passing (83% pass rate)
- Property-based testing with 100+ auto-generated test cases
- Formal verification confirming contract invariants
- Visual evidence via browser automation screenshot
- Detailed coverage analysis identifying untested code paths

**Next Steps:**
1. Resolve INC-001 (title length validation)
2. Increase coverage to 80%+ by testing remaining endpoints
3. Integrate tests into CI/CD pipeline
4. Expand contracts to cover more business logic

The artifacts produced (coverage reports, test results, screenshots) provide auditable evidence of quality for stakeholders and support continuous improvement of the V&V process.

---

## 10. Appendix A: Tool Installation & Usage

### pytest
- **Installation:** `pip install pytest`
- **Usage:** `pytest -v`
- **Documentation:** https://docs.pytest.org/

### Hypothesis
- **Installation:** `pip install hypothesis`
- **Usage:** Decorate test functions with `@given(st.text())`
- **Documentation:** https://hypothesis.readthedocs.io/

### coverage.py
- **Installation:** `pip install coverage`
- **Usage:**
  ```bash
  coverage run -m pytest
  coverage report -m
  coverage html  # open htmlcov/index.html
  ```
- **Documentation:** https://coverage.readthedocs.io/

### Playwright
- **Installation:**
  ```bash
  pip install playwright pytest-playwright
  python -m playwright install
  ```
- **Usage:** `pytest -v tests/test_ui_playwright.py`
- **Code Generation:** `python -m playwright codegen http://127.0.0.1:5000`
- **Documentation:** https://playwright.dev/python/

### icontract
- **Installation:** `pip install icontract`
- **Usage:**
  ```python
  from icontract import require, ensure

  @require(lambda x: x > 0)
  @ensure(lambda result: result >= 0)
  def sqrt(x):
      return x ** 0.5
  ```
- **Documentation:** https://icontract.readthedocs.io/

### CrossHair
- **Installation:** `pip install crosshair-tool`
- **Usage:** `crosshair check domain_contracts.py`
- **Documentation:** https://crosshair.readthedocs.io/

---

## 11. Appendix B: Running the Tests

### Prerequisites
```bash
# Clone the repository
git clone https://github.com/onurtacc/flask-todo-app
cd flask-todo-app

# Create virtual environment
python -m venv .venv
# Windows:
.venv\Scripts\activate
# macOS/Linux:
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
pip install pytest hypothesis coverage icontract crosshair-tool playwright pytest-playwright
python -m playwright install
```

### Running All Tests
```bash
# Start the Flask app in Terminal A
python app.py

# In Terminal B, run tests
make all  # or run commands below individually

# Individual commands:
pytest -v                              # Run all tests
coverage run -m pytest                 # Collect coverage
coverage report -m                     # View coverage summary
coverage html                          # Generate HTML report
pytest --junitxml=artifacts/junit.xml  # Generate JUnit XML
crosshair check domain_contracts.py    # Formal verification
```

### Makefile Targets
```bash
make run     # Start Flask server
make test    # Run pytest
make cov     # Generate coverage reports
make ui      # Run UI tests
make formal  # Run CrossHair
make all     # Run all tests and coverage
```

---

## 12. Appendix C: Artifacts

All test artifacts are stored in the `artifacts/` directory:

1. **coverage.xml** - Cobertura XML format for CI/CD integration
2. **junit.xml** - JUnit XML test results for CI/CD dashboards
3. **ui_after_add.png** - Screenshot of UI after adding a todo item
4. **htmlcov/** - HTML coverage report (open `htmlcov/index.html`)

---

**Report prepared by:** [Your Names]
**Last updated:** [Date]
**Version:** 1.0

---
## Full Presentation Outline (slides/SLIDES.md)

# SEN4013 V&V Project Presentation
## Flask Todo App Verification & Validation

**Team:** [Your Names]
**Date:** January 16, 2026
**Duration:** 5 minutes per person (adjust based on team size)

---

## Slide 1: Title Slide

**Title:** Software Verification & Validation of a Flask Todo Application

**Subtitle:** Comprehensive Testing with Functional, Property-Based, Formal, and UI/E2E Methods

**Team Members:**
- [Name 1]
- [Name 2]
- [Name 3]
- [Name 4]

**Date:** January 16, 2026

**Speaker Notes:**
"Good [morning/afternoon]. Our team conducted a comprehensive V&V study on a Flask-based todo application. We applied six different testing techniques to build confidence in the software's correctness, covering functional tests, property-based tests, runtime contracts, formal verification, and end-to-end UI testing. This presentation summarizes our approach, findings, and recommendations."

---

## Slide 2: What is Verification & Validation?

**Content:**
- **Verification:** "Are we building the product right?"
  - Does the software conform to its specification?
  - Checks correctness of implementation

- **Validation:** "Are we building the right product?"
  - Does the software meet user needs and expectations?
  - Checks fitness for purpose

- **Why Automate V&V?**
  - Higher confidence through repeatability
  - Catch defects early (lower cost)
  - Enable continuous integration/deployment
  - Provide auditable evidence of quality

**Visual:** Two-column comparison or flowchart showing Verification vs Validation

**Speaker Notes:**
"Verification asks 'are we building the product right?' ‚Äî does our code match the spec? Validation asks 'are we building the right product?' ‚Äî does it solve the user's problem? Automating V&V gives us repeatable evidence of quality, catches bugs early when they're cheap to fix, and supports modern CI/CD practices. In this project, we focused primarily on verification but also validated UI behavior against typical user expectations."

---

## Slide 3: System Under Test

**Content:**
**Flask Todo Application**
- Web UI for task management (add, edit, complete, delete)
- REST API at `/api/todos` (CRUD operations)
- Interactive Swagger documentation at `/apidocs`
- SQLite database for persistence

**Tech Stack:**
- Flask 1.1.1 + Flask-SQLAlchemy 2.4.1
- Flasgger (Swagger/OpenAPI)
- Python 3.12

**Why This App?**
- Real-world structure (UI + API + database)
- Multiple test surfaces (web forms, REST endpoints, database)
- Well-documented via Swagger

**Source:** https://github.com/onurtacc/flask-todo-app

**Visual:** Screenshot of the app UI or architecture diagram

**Speaker Notes:**
"Our system under test is a Flask-based todo list application. It exposes both a web UI for human users and a REST API for programmatic access, with interactive Swagger docs at /apidocs. It uses SQLite for data storage. We chose this app because it represents a realistic microservice with multiple interfaces to test, and the Swagger documentation provided a clear API contract to verify against."

---

## Slide 4: V&V Toolchain

**Content:**

| Tool | Purpose | Why We Used It |
|------|---------|----------------|
| **pytest** | Unit/functional testing | Industry standard, simple assertions, automatic discovery |
| **Hypothesis** | Property-based testing | Auto-generates edge cases, finds bugs humans miss |
| **coverage.py** | Code coverage analysis | Measures which code paths are tested (67% achieved) |
| **Playwright** | UI/E2E testing | Automates real browsers, captures screenshots for evidence |
| **icontract** | Runtime contracts | Enforces preconditions/postconditions at runtime |
| **CrossHair** | Formal verification | Uses SMT solver to prove contracts hold for all inputs |

**Visual:** Icons or logos of each tool

**Speaker Notes:**
"We assembled a toolchain that covers multiple dimensions of quality. Pytest handles functional tests with simple assertions. Hypothesis generates hundreds of edge-case inputs automatically‚Äîlike fuzzing with structure. Coverage.py measures how much code we exercised. Playwright drives a real browser for UI tests and captures screenshots. Icontract lets us write executable specifications as contracts. CrossHair uses a theorem prover to search for contract violations we might have missed. Together, these tools provide complementary evidence of correctness."

---

## Slide 5: Test Strategy & Approach

**Content:**

**Our Testing Strategy:**
1. **Functional Tests** ‚Äî Verify typical CRUD workflows via API
2. **Property-Based Tests** ‚Äî Use Hypothesis to generate 100+ inputs matching regex patterns
3. **Negative Tests** ‚Äî Check error handling (empty titles, oversized titles)
4. **UI/E2E Tests** ‚Äî Automate browser to add a todo, capture screenshot
5. **Structural Coverage** ‚Äî Measure code paths exercised (target: 70%+)
6. **Contract Enforcement** ‚Äî Define business rules as code, check at runtime
7. **Formal Verification** ‚Äî Use symbolic execution to prove contract properties

**Test Execution:**
- All tests automated and repeatable
- Run via `pytest` command
- Artifacts: coverage XML, JUnit XML, screenshots

**Speaker Notes:**
"Our strategy covered the full testing pyramid. At the base, we have unit and functional tests checking individual API endpoints. Above that, property-based tests generated diverse inputs to stress edge cases. We also tested error paths‚Äîwhat happens when users send bad data. At the top, UI tests validate the complete user journey through a real browser. We also layered in structural coverage to find untested code, and formal methods to mathematically verify business rules. Everything is automated‚Äîone command runs the entire suite and generates reports."

---

## Slide 6: Functional Tests

**Content:**

**Test Cases:**
- ‚úÖ T01: Home UI loads (HTTP 200)
- ‚úÖ T02: Create todo via API, verify it appears in list
- ‚úÖ T03: Reject empty title (HTTP 400)

**Example Test (pytest):**
```python
def test_home_ui_loads(client):
    r = client.get("/")
    assert r.status_code == 200
```

**Results:**
- 3/3 functional tests passed
- Verified core CRUD operations
- Confirmed error handling for empty titles

**Speaker Notes:**
"We started with functional tests covering the happy paths: loading the UI, creating a todo via the API, and listing todos. We also tested a negative case‚Äîsending an empty title‚Äîand confirmed the API rejects it with HTTP 400. These tests are straightforward but essential. They verify that basic workflows function as expected and that the API follows good HTTP semantics. All three passed."

---

## Slide 7: Property-Based Tests (Hypothesis)

**Content:**

**What is Property-Based Testing?**
- Instead of writing individual test cases, define **properties** (rules) the system must satisfy
- Hypothesis generates hundreds of inputs automatically
- If a property fails, Hypothesis shrinks the input to the minimal failing case

**Our Property:**
"For any valid title (1-40 chars, alphanumeric + punctuation), POST should succeed and GET should return it"

**Test Code:**
```python
@given(
    title=st.from_regex(r"[A-Za-z0-9 ,.!?-]{1,40}", fullmatch=True),
    desc=st.from_regex(r"[A-Za-z0-9 ,.!?-]{0,120}", fullmatch=True) | st.none()
)
def test_create_list_roundtrip(client, title, desc):
    r = client.post("/api/todos", json={"title": title, "description": desc})
    assert r.status_code in (200, 201)
    r = client.get("/api/todos")
    assert any(item["title"] == title for item in r.get_json())
```

**Results:**
- ‚úÖ 100+ generated test cases passed
- Covered edge cases: single-char titles, max-length titles, special characters

**Speaker Notes:**
"Property-based testing is a powerful technique. Instead of writing 'test with title equals hello', we say 'for ANY title matching this pattern, the system should behave correctly.' Hypothesis then generates hundreds of examples: single-letter titles, max-length titles, titles with punctuation, etc. Our test ran 100+ cases and all passed. This gives us much stronger confidence than hand-written examples‚ÄîHypothesis often finds edge cases humans wouldn't think of."

---

## Slide 8: Runtime Contracts (icontract)

**Content:**

**What are Contracts?**
- Formal specifications attached to functions
- **Preconditions:** Must be true before function executes
- **Postconditions:** Must be true after function executes
- Fail fast if violated (better than silent corruption)

**Our Contract: `normalize_title`**
```python
@require(lambda title: isinstance(title, str))
@require(lambda title: 1 <= len(title.strip()) <= 40)
@ensure(lambda title, result: result == title.strip())
def normalize_title(title: str) -> str:
    return title.strip()
```

**What This Guarantees:**
- Input must be a string
- After trimming, must be 1-40 chars
- Output is exactly the trimmed input

**Results:**
- ‚úÖ Contract test passed
- Enforces business rule: "titles must be 1-40 characters after normalization"

**Speaker Notes:**
"Contracts are executable specifications. The 'require' decorators define preconditions‚Äîwhat must be true before the function runs. The 'ensure' decorator defines postconditions‚Äîwhat must be true after. If a contract is violated, the program crashes immediately with a clear error, rather than silently producing bad data. Our normalize_title function enforces the business rule that titles must be 1-40 characters after trimming whitespace. This contract ran successfully in all tests, proving the function behaves correctly within its specified domain."

---

## Slide 9: Formal Verification (CrossHair)

**Content:**

**What is Formal Verification?**
- Uses mathematical techniques to **prove** properties hold for all possible inputs
- CrossHair uses symbolic execution + SMT (Satisfiability Modulo Theories) solver
- Searches for **counterexamples** that violate contracts

**How It Works:**
1. CrossHair reads the `@require` and `@ensure` decorators
2. Symbolically executes the function with all possible inputs
3. Uses Z3 theorem prover to find inputs that violate contracts
4. If none found ‚Üí proven correct within contract bounds

**Command:**
```bash
crosshair check domain_contracts.py
```

**Result:**
```
(no output = success)
```

**Interpretation:**
- ‚úÖ No counterexamples found
- The `normalize_title` function provably satisfies its contracts for all valid inputs
- Formal guarantee, not just tested examples

**Visual:** Diagram showing symbolic execution tree

**Speaker Notes:**
"Formal verification goes beyond testing by mathematically proving correctness. CrossHair uses symbolic execution‚Äîit doesn't run the function with concrete values, but with symbolic 'any string' inputs. It then uses a theorem prover called Z3 to search for inputs that would violate the contracts. If it finds one, it reports a counterexample. If it doesn't, that's a proof the function is correct within its contract bounds. When we ran CrossHair on our normalize_title function, it found no counterexamples, meaning we have a formal guarantee the function works correctly for all inputs satisfying the preconditions. This is stronger evidence than testing could ever provide."

---

## Slide 10: Structural Coverage Analysis

**Content:**

**Coverage Results:**
```
Name                           Stmts   Miss  Cover
----------------------------------------------------
app.py                            95     45    53%
tests/*                           35      0   100%
domain_contracts.py                6      0   100%
----------------------------------------------------
TOTAL                            136     45    67%
```

**What Was Covered:**
- ‚úÖ API create and list endpoints
- ‚úÖ Home UI route
- ‚úÖ Error handling for empty titles
- ‚úÖ All test code and contracts

**What Was NOT Covered:**
- ‚ùå Web form handlers (add, edit, delete via UI)
- ‚ùå API endpoints: GET by ID, PUT, DELETE
- ‚ùå Some error paths in validation logic

**Interpretation:**
- 67% coverage is acceptable for initial V&V
- Gaps indicate where additional tests are needed
- Test code at 100% coverage (good practice)

**Visual:** Bar chart or pie chart showing 67% coverage

**Speaker Notes:**
"Coverage measures which lines of code were executed during testing. We achieved 67% overall coverage, with 53% of the application code and 100% of our test code. The covered portions include the API endpoints we tested‚Äîcreate and list‚Äîplus error handling for empty titles. The uncovered 33% includes web form handlers and remaining API endpoints like update and delete. This coverage report tells us where our testing is strong and where we have gaps. It's not about hitting 100%‚Äîthat's often wasteful‚Äîbut about understanding what's tested and making deliberate decisions about what's not."

---

## Slide 11: UI/E2E Testing (Playwright)

**Content:**

**What is E2E Testing?**
- Tests the entire system through the user interface
- Uses a real browser (Chromium, Firefox, or WebKit)
- Simulates user actions: navigate, type, click, submit
- Captures screenshots as visual evidence

**Our Test:**
1. Launch browser
2. Navigate to http://127.0.0.1:5000
3. Fill form: title="Buy milk", description="2L semi-skimmed"
4. Click "Add" button
5. Verify "Buy milk" appears in page content
6. Capture screenshot to `artifacts/ui_after_add.png`

**Result:**
- ‚úÖ Test passed
- Todo successfully added via UI
- Screenshot captured

**Visual:** Show the screenshot from `artifacts/ui_after_add.png`

**Speaker Notes:**
"End-to-end testing validates the complete user journey through a real browser. We used Playwright to automate Chromium: it opens the app, fills in the form with 'Buy milk', clicks Add, and verifies the todo appears. This tests not just the backend logic but also the HTML templates, JavaScript, form handling, and database round-trip. The screenshot we captured is proof the UI worked correctly. This is the closest our automated tests get to how a real user experiences the application."

---

## Slide 12: Incidents & Findings

**Content:**

**Incident INC-001: Missing Title Length Validation** üî¥

**Severity:** Medium
**Status:** Unresolved

**Problem:**
- API accepts arbitrarily long titles (tested with 200 chars)
- Database schema defines `String(100)`, suggesting 100-char limit
- No validation enforces this limit in application code
- Result: Silent truncation or unexpected behavior

**Test Case:** T04 (Excessive Title Length)
- Expected: HTTP 400 with error message
- Actual: HTTP 201 Created

**Impact:**
- Data integrity: Silent data loss
- User experience: No feedback on invalid input
- Security: Potential for exploitation if database layer doesn't handle overflow gracefully

**Recommendation:**
Add validation at [app.py:152](app.py#L152):
```python
if len(title) > 100:
    return jsonify({"error": "Title must be 100 characters or less"}), 400
```

**Speaker Notes:**
"Our testing uncovered one significant defect. The API accepts titles of any length, but the database schema limits titles to 100 characters. This mismatch means the application silently truncates long titles or‚Äîdepending on the database‚Äîcould cause errors. A user trying to add a 200-character title gets an HTTP 201 success response, but their title is cut off. We classified this as medium severity: it causes data loss and poor user experience, but it's not a critical crash or security vulnerability. The fix is simple‚Äîadd a length check before creating the todo. This finding demonstrates the value of negative testing: we didn't just test happy paths, we tested what happens when users send unexpected data."

---

## Slide 13: Recommendations

**Content:**

**Immediate Actions:**
1. ‚úÖ Fix INC-001: Add title length validation (5 minutes)
2. üìà Expand test coverage to 80%+ (test remaining API endpoints)
3. üß™ Enhance property tests (Unicode, emoji, SQL injection patterns)

**Process Improvements:**
4. üîÑ **CI/CD Integration:** Run tests on every commit
   ```yaml
   - pytest --cov --junitxml=junit.xml
   - coverage xml
   - crosshair check domain_contracts.py
   ```
5. üìù **Contract-Driven Development:** Add contracts to all API endpoints
6. üîç **Mutation Testing:** Use `mutmut` to verify tests catch bugs

**Long-Term Strategy:**
7. üéØ **Fuzzing:** Apply `atheris` to find crash-inducing inputs
8. üó∫Ô∏è **Model Checking:** For complex workflows, use TLA+ or Alloy

**Speaker Notes:**
"Based on our findings, we recommend three tiers of action. First, immediate fixes: resolve the title length bug‚Äîit's a five-minute fix. Second, expand coverage and enhance property tests to cover more edge cases. Third, integrate tests into CI/CD so every code change is automatically verified. For process improvements, we suggest adopting contract-driven development across the entire codebase‚Äînot just for normalize_title‚Äîand using mutation testing to ensure our tests actually catch bugs. Long-term, consider fuzzing and model checking for even stronger guarantees. These investments in V&V pay dividends by catching bugs early, reducing production incidents, and enabling faster, confident releases."

---

## Slide 14: Conclusion & Summary

**Content:**

**What We Achieved:**
- ‚úÖ 6/7 tests passed (83% pass rate)
- ‚úÖ 67% structural code coverage
- ‚úÖ 100+ property-based test cases auto-generated
- ‚úÖ Formal proof of contract correctness (CrossHair)
- ‚úÖ Visual evidence via UI screenshot
- ‚úÖ 1 medium-severity defect identified (INC-001)

**Deliverables:**
- Comprehensive test suite (functional, property, contract, E2E)
- Coverage reports (XML, HTML)
- JUnit test results XML
- Incident report with reproduction steps and fix recommendation
- Full V&V report documenting approach, results, and recommendations

**Key Takeaway:**
"Multiple testing techniques provide complementary evidence. Unit tests catch basic bugs, property tests find edge cases, formal methods prove correctness, and E2E tests validate user experience. Together, they build confidence that the software works as intended."

**Thank You!**
Questions?

**Speaker Notes:**
"To conclude: we applied six V&V techniques to the Flask todo app, achieving 83% test pass rate and 67% code coverage. We auto-generated over 100 test cases with Hypothesis, formally proved our contracts with CrossHair, and captured visual evidence via Playwright. We identified one input validation defect and provided a fix. This multi-faceted approach‚Äîfunctional, property, formal, and E2E‚Äîgives stakeholders strong confidence in the application's correctness. No single technique is perfect, but together they cover each other's blind spots. Thank you. We're happy to take questions."

---

## Backup Slides (Optional - For Q&A)

### Backup Slide 1: Why Not Just Use Unit Tests?

**Content:**
- **Unit tests:** Good for verifying individual functions, but miss integration issues
- **Property tests:** Find edge cases humans wouldn't think to test
- **Formal verification:** Proves correctness for infinite input space, not just examples
- **E2E tests:** Validate the entire system including UI, templates, database
- **Coverage analysis:** Identifies gaps in testing

**Bottom Line:** No single technique is sufficient. V&V requires multiple perspectives.

---

### Backup Slide 2: Tool Comparison

| Technique | Pros | Cons | When to Use |
|-----------|------|------|-------------|
| Functional (pytest) | Easy to write, fast, good for regressions | Only tests examples you think of | Always‚Äîfoundation of test suite |
| Property-based (Hypothesis) | Finds unexpected edge cases | Can be slow, requires property definitions | When input space is large |
| Formal (CrossHair) | Mathematical proof of correctness | Limited to pure functions, can be slow | For critical business logic |
| E2E (Playwright) | Tests real user experience | Slow, brittle if UI changes often | For key user workflows |
| Coverage (coverage.py) | Shows untested code | Doesn't measure test quality | To identify gaps |

---

### Backup Slide 3: Challenges We Faced

1. **Python 3.12 Compatibility:** Flasgger 0.9.5 used deprecated `imp` module ‚Üí upgraded to 0.9.7.1
2. **Import Issues:** `app.py` ran `app.run()` on import ‚Üí wrapped in `if __name__ == "__main__"`
3. **UI Test Selectors:** Had to inspect HTML to find correct input names for Playwright
4. **CrossHair Performance:** Formal verification can be slow for complex functions (ours was fast)

**Lesson:** V&V tools require environment setup and debugging, but the effort pays off in confidence.

---

### Backup Slide 4: References & Documentation

**Tools:**
- pytest: https://docs.pytest.org/
- Hypothesis: https://hypothesis.readthedocs.io/
- coverage.py: https://coverage.readthedocs.io/
- Playwright: https://playwright.dev/python/
- icontract: https://icontract.readthedocs.io/
- CrossHair: https://crosshair.readthedocs.io/

**Project:**
- GitHub: https://github.com/onurtacc/flask-todo-app
- Our Report: [REPORT.md](REPORT.md)

**Further Reading:**
- Dijkstra (1970): "Testing shows the presence, not the absence of bugs"
- Formal Methods in Industry: Microsoft's use of TLA+ for Azure services
- Property-Based Testing: QuickCheck (Haskell), ScalaCheck, fast-check (JavaScript)

---

**END OF PRESENTATION**

---

## Presentation Tips

**Timing:**
- 12 slides = ~2.5 minutes per slide for a 30-minute presentation
- Adjust if team size requires different timing (e.g., 5 min/person √ó 4 people = 20 min + 10 min Q&A)

**Delivery:**
- Practice with timer‚Äîdon't rush slide 7 (property-based) and slide 9 (formal), as they're conceptually dense
- Use speaker notes‚Äîdon't just read slides
- Show the screenshot on slide 11, show htmlcov on slide 10 if possible (live demo or screenshot)
- Incident slide (12) is key for showing V&V value‚Äîemphasize the finding

**Visual Aids:**
- Include screenshots of htmlcov, Swagger UI, app UI, or test output
- Use icons/logos for tools (pytest, Playwright, etc.)
- Consider live demo: run `pytest -v` or open `htmlcov/index.html`

**Engagement:**
- Ask audience: "Who has used property-based testing before?" (probably few‚Äîgood teaching moment)
- Q&A: Prepare for "Why didn't you test X?" ‚Üí answer: "We prioritized based on risk; that's a good candidate for future work"

**Export to PDF:**
- Use Markdown-to-slides tools: Marp, reveal.js, or Google Slides
- If using PowerPoint: paste content and add visuals

Good luck! üöÄ


